# Predictive Analytics and Data Obfuscation for Insurance Benefits

## Project Overview
The **Sure Tomorrow Insurance Company** is using machine learning (ML) and linear algebra (LA) to enhance its customer insights, predict insurance benefits, and ensure data privacy. This project demonstrates the use of ML algorithms to:
1. Identify customers who are similar to a given customer for marketing purposes.
2. Predict whether a customer is likely to receive an insurance benefit.
3. Estimate the number of insurance benefits a new customer is likely to receive using linear regression.
4. Protect clients' personal data through obfuscation techniques, ensuring data privacy without compromising the model's performance.

## Project Structure

1. **Introduction**
2. **Data Preprocessing & Exploration**
3. **Task 1: Find Similar Customers**
4. **Task 2: Predict Likelihood of Insurance Benefit**
5. **Task 3: Predict Number of Insurance Benefits (Linear Regression)**
6. **Task 4: Data Obfuscation and Privacy Protection**
7. **Conclusion**

## 1. Introduction
This project aims to explore predictive analytics and data security techniques by:
- Analyzing customer data to predict insurance benefits.
- Implementing a data obfuscation algorithm to protect sensitive customer information while maintaining model accuracy.

The dataset includes the following features:
- **gender**: Gender of the customer.
- **age**: Age of the customer.
- **income**: Income of the customer.
- **family_members**: Number of family members.
- **insurance_benefits**: Number of insurance benefits received in the past 5 years.

The target variable is **insurance_benefits** (number of benefits).

## 2. Data Preprocessing & Exploration
We begin by loading and cleaning the data:
- **Handle missing values**: There are no missing values in the dataset.
- **Remove duplicates**: Duplicate rows were removed, leaving 5000 unique customer entries.
- **Feature transformations**: Some features, like `age`, were transformed for consistency.

Basic descriptive statistics were generated to understand the distribution of features. Data visualizations like pair plots were also used to inspect relationships between variables.

## 3. Task 1: Find Similar Customers
We implemented a k-Nearest Neighbors (kNN) algorithm to identify customers similar to a given customer. The task involved:
- Testing the algorithm with different scaling techniques (unscaled vs. MaxAbsScaler) and distance metrics (Euclidean vs. Manhattan).
- **Scaling**: Scaling significantly impacted the performance of kNN, particularly when using Euclidean distance, as features with larger numerical ranges dominated distance calculations.
- **Findings**: Scaling improved performance, especially for small values of k (e.g., k=1, k=3, k=9).

Key takeaway: **Scaling is important** for kNN models, especially with Euclidean distance.

## 4. Task 2: Predict Likelihood of Insurance Benefit
This task involved building a kNN classifier to predict whether a customer is likely to receive an insurance benefit. The task was formulated as a **binary classification problem**:
- We used **F1-score** to evaluate model performance.
- **Class Imbalance**: The dataset was highly imbalanced (88.38% of customers did not receive benefits), so the F1-score was a better metric than accuracy.
- **Results**: The kNN classifier outperformed a dummy classifier. Scaling the data improved performance significantly, with F1-scores ranging from **0.91 to 0.95** for scaled data.

Key takeaway: **Scaling improves kNN performance**, especially when addressing class imbalance.

## 5. Task 3: Predict Number of Insurance Benefits (Linear Regression)
We implemented Linear Regression using matrix operations to predict the number of insurance benefits a customer is likely to receive:
- We used **Root Mean Squared Error (RMSE)** and **R² score** for model evaluation.
- **Findings**:
  - RMSE: **0.36**
  - R² score: **0.43** (indicating moderate fit)
  - **Scaling**: No significant difference was observed in performance between scaled and unscaled data, as Linear Regression adjusts automatically to feature scales.

Key takeaway: **Scaling does not impact Linear Regression** performance with the normal equation.

## 6. Task 4: Data Obfuscation and Privacy Protection
We implemented a **data obfuscation** technique to protect customer information:
- A random **invertible matrix (P)** was used to transform the data.
- **Key steps**:
  - Obfuscated data was generated by multiplying the feature matrix by the matrix \( P \).
  - The obfuscated data could be recovered using the inverse of \( P \).
  - **Analytical proof**: The obfuscation did not affect model performance (RMSE or R² score) as the predictions remained the same after obfuscation.
  - **Results**: Both original and obfuscated data produced identical results for Linear Regression, confirming that obfuscation preserves the model's predictive power.

Key takeaway: **Data obfuscation** using matrix multiplication ensures privacy while preserving model effectiveness.

## 7. Conclusion
This project demonstrated how machine learning can be used to predict insurance benefits while ensuring data privacy through obfuscation. The key findings include:
- **Scaling** improves kNN performance, especially for distance-based metrics like Euclidean distance.
- **Class imbalance** can be managed by using appropriate metrics like F1-score for classification tasks.
- **Linear regression** provides a good baseline model, but more advanced models can improve predictive accuracy.
- **Data obfuscation** effectively protects sensitive data without compromising model performance, making it suitable for real-world applications in data-sensitive domains.

## How to Run the Project

### Prerequisites:
Install the required libraries:
```bash
pip install numpy pandas scikit-learn seaborn matplotlib
